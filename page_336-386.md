### [问题]

#### 序列建模部分

**1.循环神经网络：RNN**

- 参数共享

好处：

1)能够使模型扩展

2)并且可带来泛化

举例：

I went tot Nepal in 2009

In 2009, I went to Nepal

上述两段中，2009年，应当被快速识别出来 → 循环神经网络可以做到

- 可将过去&未来关联起来

- 设计模式

1)每个时间步都有输出

2)每个时间步都产生一个输出

3)隐藏单元之间存在循环连接

- 计算成本：各个参数的损失函数梯度计算

1)执行一次前向传播

2)由右到做的反向传播

3)且不同通过并行化来降低计算成本

4)前向传播中的各个状态必须保存

5)直到他们在反向传播中被再次使用

- 计算循环神经网络的梯度：BRTT

一旦获得了计算图内部节点的梯度，就可以得到关于参数节点的梯度，因为参数在许多时间步共享

- 基于上下文的RNN序列建模


**2.其他RNN**

- 双向RNN

输出结果Y取决于未来的输入，如：当前这个声音音素取决于未来的几个词，这种情况

双向循环网络可解决此类场景；如手写识别、生物信息学

- 基于编码-解码的序列

输入 → 输出的关联

1)输入序列，映射成 → 固定大小的向量

2)向量，映射成 → 序列

3)输入序列，映射成 → 等长输出序列

4)输入序列，映射成 → 不等长输出序列；如语音识别、机器翻译或问答

只因多出了一个上下文变量C

- 深度循环网络

1)从输入到隐藏状态

2)从前一个隐藏状态到下一个隐藏状态

3)从隐藏状态到输出

引入深度 → 每一块可各自使用一个单独的MLP

- 递归神经网络

深的树状结构

优势：对具有相同长度的序列，深度可以急剧的减少


**3.解决长期依赖**

- 长期依赖问题的原因

经过许多阶段传播后的梯度倾向于消失 or 爆炸

- 如何解决？

1)每个时刻使用不同权重w

2)设定循环隐藏单元

3)加入储层计算

4)谱半径？？

5)细粒度 + 粗粒度上使用不同的模型：渗漏单元？？

6)长短期记忆：LSTM

7)跟二阶导数起详细作用的Nesterov动量法？？


**4.外显记忆：想记住事实**

- 引入记忆网络

基于内容的寻址 → 通过几个歌词回忆起一首歌曲

基于位置的寻址

