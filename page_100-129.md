### [归纳]

#### 数值计算部分

1.机器学习算法需要大量的运算
- 多数为迭代过程所产生的运算
- 为了找到最小化 or 最大化函数值的参数
- 是否也可以往求解线性方程组的方向走一走？

2.由于涉及很多计算，故对数值很敏感
- 接近于0，被四舍五入为0 → 下溢
- 超大的数，被近似为♾ → 上溢
- 上述两种情况可被视为数值的不稳定，但在算法中是需要保持稳定的
- 引入了softmax函数，来解决上溢 + 下溢的问题

3.希望设计出的函数稳定
- 当输入值有微小的波动时，不希望引起结果值的波动
- 希望函数具有低的敏感性，从而带来模型的稳定性
- 但矩阵本身就自带些许的敏感性，有待注意 + 探查

4.最大值 + 最小值
- 为什么要求最大值 + 最小值
- 在数学层面，求最值是一个很普遍的诉求
- 求最大成功率、求最低证券指数、等等...
- 恰巧，在机器学习中，拟合函数与真实值的差 → 误差函数值要最小

5.优化方法
- 意义在于通过不停的迭代，去寻找使误差函数值最小的参数值
- 即去寻找使代价函数最小的参数值们
- 涉及到如何快速去找、如何准确的去找
- 涉及到一阶导
- 二阶导则会引入曲率，衡量一阶导的变化：越来越快的变大、越来越慢的变小
- 常用方法为基于梯度的优化方法 → 梯度下降
- 梯度下降也有自身的局限性：被限制在连续空间上的优化
- 在此基础上加入x的值限定范围，即就成了约束优化

6.将基于梯度的优化方法代入到矩阵中
- 引申出偏导数、学习率、一阶优化、二阶最优化、凸优化等等

-----

#### 机器学习基础部分

1.机器学习基础中所涉及到的点
- 拟合 → 泛化
- 隶属于应用统计学
- 流程为优化算法、代价函数、模型、数据集
- 哪些是限制机器学习泛化能力的因素？

2.概率分布与无监督算法有什么关系？
- 概率分布为样本数据集所带的属性之一

3.误差
- 均方误差
- 训练误差
- 泛化误差

4.机器学习的主要挑战 → 泛化能力
- 泛化能力与容量 + 样本集数量 + 模型有关
- 训练集 + 测试集数据，遵循独立同分布假设
- 模型都有自身的最优容量，容量不足就加入额外特征，如(加入x的幂次项)
- 也可使用贝叶斯误差来作为参照
- 解决该类问题的思路：将学习算法嵌入到增加参数的算法中
- 使用特征越多 → 训练误差与泛化误差差异就越大
- 训练样本越多 → 训练误差与泛化误差差异就越小

5.没有免费的午餐定理
- 最终目标：
- 并非找一个通用的学习算法
- 也并非找一个绝对最好的学习算法
- 而是理解什么样的分布与“真实世界”相关
- 去找到在当前分布上效果最好的学习算法

