### [问题]

#### 深度模型中的优化问题部分

**1.学习算法**

- 应用最多的优化算法之一：随机梯度下降

关键参数：学习率

学习率应该如何选取？

学习率的不同倾向，又会对模型造成哪些影响？

学习率太小 → 学习过程很缓慢

学习率太低 → 学习可能会卡在一个相当高的代价值

理想状态：不受训练数目多寡的影响

- 优化算法的收敛率？

- 动量

动量算法的宗旨：加速学习

动量跟物理相关概念结合


**2.参数初始化策略**

- 优化算法的迭代和非迭代性？

针对于深度学习：

1)模型的训练算法通常为迭代

2)与初始值有关

当前完全确知的唯一特性：初始参数需要在不同的单元间破坏对称性

引申出来的问题：

1)初始参数相同

2)激活函数相同

3)输入相同

- 几种推荐的初始化策略

标准初始化

设置偏置的方法

反差或精确度参数

- 目的

1)获得更快的收敛率

2)更好的泛化误差

**3.自适应学习率算法**

- 学习率：难以设置的超参数之一

为什么？

因为他对模型的性能有显著的影响

- 有若干个推荐的方法
